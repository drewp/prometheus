# docs: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
# "Whenever the alert expression results in one or more vector
# elements at a given point in time, the alert counts as active for
# these elements' label sets."

# also https://www.metricfire.com/blog/top-5-prometheus-alertmanager-gotchas/#Missing-metrics

groups:
  - name: k8s
    rules:
      # from https://awesome-prometheus-alerts.grep.to/rules.html
      - alert: PrometheusTargetMissing
        expr: up == 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: Prometheus target missing (instance {{ $labels.instance }})
          description: "A Prometheus target has disappeared. An exporter might be crashed.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
      - alert: KubernetesMemoryPressure
        expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: Kubernetes memory pressure (instance {{ $labels.instance }})
          description: "{{ $labels.node }} has MemoryPressure condition\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
      - alert: KubernetesDiskPressure
        expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: Kubernetes disk pressure (instance {{ $labels.instance }})
          description: "{{ $labels.node }} has DiskPressure condition\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
      - alert: KubernetesOutOfDisk
        expr: kube_node_status_condition{condition="OutOfDisk",status="true"} == 1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: Kubernetes out of disk (instance {{ $labels.instance }})
          description: "{{ $labels.node }} has OutOfDisk condition\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
      - alert: KubernetesJobFailed
        expr: kube_job_status_failed > 0
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Kubernetes Job failed (instance {{ $labels.instance }})
          description: "Job {{$labels.namespace}}/{{$labels.exported_job}} failed to complete\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: KubernetesPodCrashLooping
        expr: increase(kube_pod_container_status_restarts_total[1m]) > 3
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Kubernetes pod crash looping (instance {{ $labels.instance }})
          description: "Pod {{ $labels.pod }} is crash looping\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
      - alert: KubernetesClientCertificateExpiresNextWeek
        expr: apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 7*24*60*60
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Kubernetes client certificate expires next week (instance {{ $labels.instance }})
          description: "A client certificate used to authenticate to the apiserver is expiring next week.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

  - name: webcam
    rules:
      - alert: twinscam_not_reporting
        expr: absent(cam_pipeline_state{job="webcam-record-twinscam"})
        for: 2m
        labels:
          severity: losingData
        annotations:
          summary: "webcam-record-twinscam is not reporting metrics {{ $labels }}"

      # - alert: cam_garagehall_not_reporting
      #   expr: absent(cam_pipeline_state{job="webcam-record-garagehall"})
      #   for: 2m
      #   labels:
      #     severity: losingData
      #   annotations:
      #     summary: "webcam-record-garagehall is not reporting metrics {{ $labels }}"

      - alert: cam_pipeline_stopped
        expr: sum without (instance) (cam_pipeline_state{cam_pipeline_state="playing"}) < 1
        for: 10m
        labels:
          severity: losingData
        annotations:
          summary: "webcam-record gst pipeline is not state=playing {{ $labels }}"

      - alert: cam_not_advancing
        expr: rate(cam_stream_bytes{element="splitmux"}[3m]) < 0.2
        for: 10m
        labels:
          severity: losingData
        annotations:
          summary: "cam output bytes is advancing too slowly. {{ $labels }}"

      - alert: webcam_indexer_stalled
        expr: rate(webcam_indexer_update_count{job="webcam-indexer"}[5m]) < .01
        for: 10m
        labels:
          severity: webcamUsersAffected
        annotations:
          summary: "webcam indexer update loop is stalled"

  - name: Outages
    rules:
      - alert: powereagleStalled
        expr: rate(house_power_w[100m]) == 0
        for: 0m
        labels:
          severity: losingData
        annotations:
          summary: "power eagle data stalled"
          description: "logs at https://bigasterisk.com/k/clusters/local/namespaces/default/deployments/power-eagle/logs"

      - alert: powereagleAbsent
        expr: absent_over_time(house_power_w[5m])
        for: 2m
        labels:
          severity: losingData
        annotations:
          summary: "power eagle data missing"
          description: "logs at https://bigasterisk.com/k/clusters/local/namespaces/default/deployments/power-eagle/logs"

      - alert: wifi_scrape_errors
        expr: rate(poll_errors_total{job="wifi"}[2m]) > .1
        labels:
          severity: houseUsersAffected
        annotations:
          summary: "errors getting wifi users list"

      - alert: absent_mitmproxy
        expr: absent(process_resident_memory_bytes{job="mitmproxy"})
        labels:
          severity: houseUsersAffected
        annotations:
          summary: "mitmproxy metrics not responding. See https://bigasterisk.com/grafana/d/ix3hMAdMk/webfilter?orgId=1&from=now-12h&to=now and https://bigasterisk.com/k/clusters/local/namespaces/default/deployments/mitmproxy (metrics actually come from webfilter.py plugin)"

      - alert: net_routes_sync
        expr: min(sync_is_up{job="net-routes"}) != 1
        for: 30m
        labels:
          severity: houseUsersAffected
        annotations:
          summary: "mitmproxy not syncing. See https://bigasterisk.com/grafana/d/ix3hMAdMk/webfilter?orgId=1&from=now-12h&to=now and https://bigasterisk.com/k/clusters/local/namespaces/default/deployments/net-routes"

      - alert: ping_bang5_to_dash5
        expr: 'ping_percent_packet_loss{host="dash"} > 10 or absent_over_time(ping_percent_packet_loss{host="dash"}[5m])'
        for: 5m

      - alert: ping_bang5_to_frontbed5
        expr: 'ping_percent_packet_loss{host="frontbed"} > 10 or absent_over_time(ping_percent_packet_loss{host="frontbed"}[5m])'
        for: 5m
      
      # sync to netdevices.n3
  - name: alerts
    rules:
      - { alert: housePower, expr: "house_power_w > 3000", for: 20m, labels: { severity: waste }, annotations: { summary: "house power usage over 3KW" } }
      - alert: ssl_certs_expiring_soon
        expr: min((min_over_time(probe_ssl_earliest_cert_expiry[1d])-time())/86400) < 10
        labels:
          severity: futureUsersAffected
        annotations:
          summary: "cert expiring soon. See https://bigasterisk.com/grafana/d/z1YtDa3Gz/certs?orgId=1\nVALUE = {{ $value }}\n  LABELS = {{ $labels }}"
